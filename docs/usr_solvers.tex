%=============================================================================
%=============================================================================

\chapter{Solvers and Preconditioners}
\label{ch-Solvers}

There are several solvers available in \hypre{} via different
conceptual interfaces (see Table \ref{table-solver-availability}).
The procedure for setup and use of solvers and preconditioners is
largely the same. We will refer to them both as solvers in the sequel
except when noted.  In normal usage, the preconditioner is chosen and
constructed before the solver, and then handed to the solver as part
of the solver's setup.  In the following, we assume the most common
usage pattern in which a single linear system is set up and then
solved with a single righthand side. We comment later on
considerations for other usage patterns.

\begin{table}[h]
\center
\begin{tabular}{|l||c|c|c|c|}
\hline
                               & \multicolumn{4}{|c|}{System Interfaces} \\
\multicolumn{1}{|c||}{Solvers} & Struct & SStruct & FEI & IJ \\
\hline\hline
Jacobi     & X & X &   &   \\
SMG        & X & X &   &   \\
PFMG       & X & X &   &   \\
SysPFMG    &   & X &   &   \\
Split      &   & X &   &   \\
FAC        &   & X &   &   \\
Maxwell    &   & X &   &   \\
AMS        &   & X & X & X \\
BoomerAMG  &   & X & X & X \\
MLI        &   & X & X & X \\
ParaSails  &   & X & X & X \\
Euclid     &   & X & X & X \\
PILUT      &   & X & X & X \\
PCG        & X & X & X & X \\
GMRES      & X & X & X & X \\
BiCGSTAB   & X & X & X & X \\
Hybrid     & X & X & X & X \\
\hline
\end{tabular}
\caption{%
Current solver availability via \hypre{} conceptual interfaces.
}
\label{table-solver-availability}
\end{table}

%-----------------------------------------------------------------------------

\section*{Setup:}

\begin{enumerate}

\item
{\bf Pass to the solver the information defining the problem.} In the
typical user cycle, the user has passed this information into a matrix
through one of the conceptual interfaces prior to setting up the
solver. In this situation, the problem definition information is then
passed to the solver by passing the constructed matrix into the
solver. As described before, the matrix and solver must be compatible,
in that the matrix must provide the services needed by the
solver. Krylov solvers, for example, need only a matrix-vector
multiplication.  Most preconditioners, on the other hand, have
additional requirements such as access to the matrix coefficients.

\item
{\bf Create the solver/preconditioner} via the \code{Create()} routine.

\item
{\bf Choose parameters for the preconditioner and/or solver.}
Parameters are chosen through the \code{Set()} calls provided by the
solver.  Throughout \hypre{}, we have made our best effort to
give all parameters reasonable defaults if not chosen.  However, 
for some preconditioners/solvers the best choices for parameters
depend on the problem to be solved. We give recommendations in the
individual sections on how to choose these parameters.
Note that in \hypre{}, convergence
criteria can be chosen after the preconditioner/solver has been setup.
For a complete set of all available parameters see the Reference Manual.

\item
{\bf Pass the preconditioner to the solver.} For solvers that are not
preconditioned, this step is omitted.  The preconditioner is passed
through the \code{SetPrecond()} call.

\item
{\bf Set up the solver.} This is just the \code{Setup()} routine.
At this point the matrix and right hand side is passed into the solver
or preconditioner. Note that the actual right hand side is not used
until the actual solve is performed.

\end{enumerate}

At this point, the solver/preconditioner is fully constructed and
ready for use.

%-----------------------------------------------------------------------------

\section*{Use:}

\begin{enumerate}

\item
{\bf Set convergence criteria.}  Convergence can be controlled by the
number of iterations, as well as various tolerances such as relative
residual, preconditioned residual, etc.  Like all parameters,
reasonable defaults are used.  Users are free to change these, though
care must be taken.  For example, if an iterative method is used as a
preconditioner for a Krylov method, a constant number of iterations is
usually required.

\item
{\bf Solve the system.}  This is just the \code{Solve()} routine.

\end{enumerate}

%-----------------------------------------------------------------------------

\section*{Finalize:}

\begin{enumerate}

\item
{\bf Free the solver or preconditioner.} This is done using the
\code{Destroy()} routine.

\end{enumerate}

%-----------------------------------------------------------------------------

\section* {Synopsis}

In general, a solver (let's call it PHOO) is set up and run using the following routines,
where A is the matrix, b the right hand side and x the solution vector
of the linear system to be solved:

\begin{display}
\begin{verbatim}
/* Create Solver */
int HYPRE_PHOOCreate(MPI_COMM_WORLD, &solver); 

/* set certain parameters if desired */
HYPRE_PHOOSetTol(solver, 1.e-8);
.
.
/* Set up Solver */
HYPRE_PHOOSetup(solver, A, b, x);
/* Solve the system */
HYPRE_PHOOSolve(solver, A, b, x);
/* Destroy the solver */
HYPRE_PHOODestroy(solver);
\end{verbatim}
\end{display}

In the following sections, we will give brief descriptions of the available \hypre{} solvers
with some suggestions on how to choose the parameters as well as references for users 
who are interested in a more detailed description and analysis of the solvers.
A complete list of all routines that are available can be found in the reference manual.

\section{SMG}

SMG is a parallel semicoarsening multigrid solver for the linear
systems arising from finite difference, finite volume, or finite
element discretizations of the diffusion equation,
\begin{equation}
\nabla \cdot ( D \nabla u ) + \sigma u = f
\end{equation}
on logically rectangular grids.  The code solves both 2D and 3D
problems with discretization stencils of up to 9-point in 2D and up to
27-point in 3D.  See
\cite{SSchaffer_1998a,PNBrown_RDFalgout_JEJones_2000,RDFalgout_JEJones_2000}
for details on the algorithm and its parallel implementation/performance.

SMG is a particularly robust method.  The algorithm semicoarsens in
the z-direction and uses plane smoothing.  The xy plane-solves are
effected by one V-cycle of the 2D SMG algorithm, which semicoarsens in
the y-direction and uses line smoothing.

%-----------------------------------------------------------------------------

\section{PFMG}

PFMG is a parallel semicoarsening multigrid solver similar to SMG.
See \cite{SFAshby_RDFalgout_1996,RDFalgout_JEJones_2000} for details
on the algorithm and its parallel implementation/performance.

The main difference between the two methods is in the smoother: PFMG
uses simple pointwise smoothing.  As a result, PFMG is not as robust
as SMG, but is much more efficient per V-cycle.

%-----------------------------------------------------------------------------
                                                                                                                                                             
\section{SysPFMG}

SysPFMG is a parallel semicoarsening multigrid solver for systems of 
elliptic PDEs. It is a generalization of PFMG, with the interpolation
defined only within the same variable. The relaxation is of nodal type-
all variables at a given point location are simultaneously solved for in the
relaxation.

Although SysPFMG is implemented through the SStruct interface, it can
be used only for problems with one grid part. Ideally, the solver should
handle any of the seven variable types (cell-, node-, xface-, yface-, zface-,
xedge-, yedge-, and zedge-based). However, it has been completed only for cell-based
variables.

%-----------------------------------------------------------------------------

%-----------------------------------------------------------------------------
                                                                                                                                                             
\section{SplitSolve}
                                                                                                                                                             
SplitSolve is a parallel block Gauss-Seidel solver for semi-structured
problems with multiple parts. For problems with only one variable, it can be viewed as 
a domain-decomposition solver with no grid overlapping.

Consider a multiple part problem given by the linear system $Ax=b$. Matrix $A$ can
be decomposed into a structured intra-variable block diagonal component $M$ and a
component $N$ consisting of the inter-variable blocks and any unstructured connections
between the parts. SplitSolve performs the iteration 
\[ x_{k+1} = \tilde{M}^{-1} (b + N x_k),\]
where $\tilde{M}^{-1}$ is a decoupled block-diagonal V(1,1) cycle, a separate cycle for each
part and variable type. There are two V-cycle options, SMG and PFMG.
%-----------------------------------------------------------------------------

\section{Hybrid}

The hybrid solver is designed to detect whether a multigrid preconditioner
is needed when solving a linear system.
It first uses a diagonally scaled Krylov solver, which can be chosen by the user
(the default is conjugate gradient, but one should use GMRES if the matrix of the 
linear system to be solved is nonsymmetric). It monitors how fast the Krylov solver
converges.
If there is not sufficient progress, the algorithm switches to a preconditioned
Krylov solver.

This solver is both available through the structured and the ij interface,
however under a different name.

If used through the structured interface, it is called StructHybrid, and can be
used with the preconditioners PFMG and SMG.

If used through the IJ interface, it is called ParCSRHybrid and can be used with 
the preconditioner BoomerAMG.


%-----------------------------------------------------------------------------

\section{BoomerAMG}

BoomerAMG is a parallel implementation of the algebraic multigrid 
method \cite{Ruge_Stueben_1987}. 
It can be used
both as a solver or as a preconditioner.  The user can choose between various
different parallel coarsening techniques, interpolation and relaxation schemes.
See
\cite{VEHenson_UMYang_2002,UMYang_2005} for a detailed description of the 
coarsening
algorithms, interpolation and relaxation schemes as well as numerical results.  The following
coarsening techniques are available:
\begin{itemize}
\item the Cleary-Luby-Jones-Plassman (CLJP) coarsening,
\item the Falgout coarsening which is a combination of CLJP and the
classical RS coarsening algorithm,
\item PMIS and HMIS coarsening algorithms which lead to coarsenings with lower complexities \cite{DeSterck_Yang_Heys_2004}
and 
\item aggressive coarsening, which can be applied to any of the coarsening techniques mentioned above and thus achieving much lower complexities and lower memory use \cite{Stueben_1999}.
\end{itemize}
The following interpolation techniques are available:
\begin{itemize}
\item the ``classical" interpolation as defined in \cite{Ruge_Stueben_1987},
\item direct interpolation \cite{Stueben_1999},
\item multipass interpolation \cite{Stueben_1999}.
\item the ``classical" interpolation modified for hyperbolic PDEs.
\end{itemize}

The following relaxation techniques are available:
\begin{itemize}
\item weighted Jacobi relaxation,
\item a hybrid Gauss-Seidel / Jacobi relaxation scheme, 
\item a symmetric hybrid Gauss-Seidel / Jacobi relaxation scheme, and
\item hybrid block smoothers \cite{UMYang_2004}.
\end{itemize}

If the users wants to solve systems of PDEs and can provide information on
which variables belong to which function, BoomerAMG's systems AMG version
can also be used.

%-----------------------------

\subsection{Parameter settings}

BoomerAMG's Create function differs from the synopsis in that it has only one parameter
int HYPRE$\_$BoomerAMGCreate(HYPRE$\_$Solver *solver). It uses the communicator 
of the matrix A.

For best performance, it might be necessary to set certain parameters.
One important parameter is the strong threshold, which can be set
using the function HYPRE$\_$BoomerAMGSetStrongThreshold.
The default value is 0.25, which appears to be a good choice for 2-dimensional
problems. A better choice for 3-dimensional problems appears to be 0.5. However,
the choice of the strength threshold is problem dependent and therefore
there could be better choices than the two suggested ones.
In some cases (such as a 3D 7-point Laplace problem) good performance
could be obtained by setting the strength threshold to 0 and additionally
setting the interpolation truncation factor to 0.3 
via HYPRE$\_$BoomerAMGSetTruncFactor.

For a complete listing of all parameters see the reference manual.
%-----------------------------------------------------------------------------

\section{ParaSails}

ParaSails is a parallel implementation of a sparse approximate inverse
preconditioner, using {\em a priori} sparsity patterns and least-squares
(Frobenius norm) minimization.  Symmetric positive definite (SPD) problems
are handled using a factored SPD sparse approximate inverse.  General
(nonsymmetric and/or indefinite) problems are handled with an
unfactored sparse approximate inverse.  It is also possible to
precondition nonsymmetric but definite matrices with a factored, SPD
preconditioner.

ParaSails uses {\em a priori} sparsity patterns that are patterns of powers
of sparsified matrices.  ParaSails also uses a post-filtering technique
to reduce the cost of applying the preconditioner.  
In advanced usage not described here, the pattern of the
preconditioner can also be reused to generate preconditioners for different
matrices in a sequence of linear solves.

For more details about the ParaSails algorithm, see \cite{EChow_2000}.

%-----------------------------

\subsection{Parameter Settings}

The accuracy and cost of ParaSails are parameterized by the real {\em thresh}
and integer {\em nlevels} parameters,
$0 \le {\it thresh} \le 1$, $0 \le {\it nlevels}$.
Lower values of {\em thresh}
and higher values of {\em nlevels} lead to more accurate, but more expensive
preconditioners.  More accurate preconditioners are also more expensive
per iteration.  The default values are ${\it thresh} = 0.1$
and ${\it nlevels} = 1$.  The parameters are set using
\code{HYPRE_ParaSailsSetParams}.

Mathematically, given a symmetric matrix $A$, the pattern of the
approximate inverse is the pattern of $\tilde{A}^m$ where $\tilde{A}$
is a matrix that has been sparsified from $A$.  The sparsification
is performed by dropping all entries in a symmetrically diagonally scaled $A$
whose values are less than {\em thresh} in magnitude.  The parameter
{\em nlevel} is equivalent to $m+1$.
Filtering is a post-thresholding procedure.
For more details about the algorithm, see \cite{EChow_2000}.

The storage required for the ParaSails preconditioner depends on
the parameters {\em thresh} and {\em nlevels}.  The default parameters
often produce a preconditioner that can be stored in less than the
space required to store the original matrix.
ParaSails does not need a large amount of intermediate storage in
order to construct the preconditioner.

%-----------------------------

ParaSail's Create function differs from the synopsis in the followin way:

\begin{display}
\begin{verbatim}
int HYPRE_ParaSailsCreate(MPI_Comm comm, HYPRE_Solver *solver,
  int symmetry);
\end{verbatim}
\end{display}
where \code{comm} is the MPI communicator.

The value of \code{symmetry} has the following meanings, to indicate
the symmetry and definiteness of the problem, and to specify the 
type of preconditioner to construct:
\begin{center}
\begin{tabular}{|c|l|} \hline
value & meaning \\ \hline
0 & nonsymmetric and/or indefinite problem, and nonsymmetric preconditioner \\
1 & SPD problem, and SPD (factored) preconditioner \\
2 & nonsymmetric, definite problem, and SPD (factored) preconditioner \\ 
\hline
\end{tabular}
\end{center}
For more information about the final case, see section \ref{nearly}.

Parameters for setting up the preconditioner are specified using
\begin{display}
\begin{verbatim}
int HYPRE_ParaSailsSetParams(HYPRE_Solver solver, 
  double thresh, int nlevel, double filter);
\end{verbatim}
\end{display}

The parameters are used to specify the sparsity pattern and filtering value
(see above), and are described with suggested values as follows:

\begin{center}
\begin{tabular}{|c|c|c|c|c|l|} \hline
parameter    & type    & range                & sug. values  & default & meaning \\ \hline
{\tt nlevel} & integer & ${\tt nlevel} \ge 0$ & 0, 1, 2      & 1   & $m={\tt nlevel}+1$\\
\hline
{\tt thresh} & real    & ${\tt thresh} \ge 0$ & 0, 0.1, 0.01 & 0.1 & {\em thresh} $=$ {\tt thresh}\\
             &         & ${\tt thresh}  <  0$ & -0.75, -0.90 &     & {\em thresh} selected automatically\\
\hline
{\tt filter} & real    & ${\tt filter} \ge 0$ & 0, 0.05, 0.001 & 0.05 & filter value $=$ {\tt filter}\\
             &         & ${\tt filter}  <  0$ & -0.90        &     & filter value selected automatically\\
\hline
\end{tabular}
\end{center}

When ${\tt thresh} < 0$, then a threshold is selected such that 
$-{\tt thresh}$ represents the fraction of the nonzero elements
that are dropped.  For example, if ${\tt thresh} = -0.9$ then
$\tilde{A}$ will contain approximately ten percent of the nonzeros
in $A$.

When ${\tt filter} < 0$, then a filter value is selected such that 
$-{\tt filter}$ represents the fraction of the nonzero elements
that are dropped.  For example, if ${\tt filter} = -0.9$ then
approximately 90 percent of the entries in the computed approximate 
inverse are dropped.

%-----------------------------

\subsection{Preconditioning nearly symmetric matrices}
\label{nearly}

A nonsymmetric, but definite and nearly symmetric matrix $A$ 
may be preconditioned
with a symmetric preconditioner $M$.  Using a symmetric preconditioner
has a few advantages, such as guaranteeing positive
definiteness of the preconditioner, as well as being less expensive
to construct.

The nonsymmetric matrix $A$ must be definite,
i.e., $(A+A^T)/2$ is SPD, and the {\em a priori} sparsity pattern to be used
must be symmetric.  The latter may be guaranteed by 1) 
constructing the sparsity pattern with a symmetric matrix, or 2) if the
matrix is structurally symmetric (has symmetric pattern), then
thresholding to construct the pattern is not used (i.e.,
zero value of the {\tt thresh} parameter is used).

%-----------------------------------------------------------------------------

\section{Euclid}

The Euclid library is a scalable implementation of the Parallel ILU algorithm
that was presented at SC99~\cite{DHysom_APothen_1999}, and published in
expanded form in the SIAM Journal on Scientific
Computing~\cite{DHysom_APothen_2001}.  By {\em scalable} we mean that the
factorization (setup) and application (triangular solve) timings remain nearly
constant when the global problem size is scaled in proportion to the number of
processors.  As with all ILU preconditioning methods, the number of iterations
is expected to increase with global problem size.

Experimental results have shown that PILU preconditioning is in general
more effective than Block Jacobi preconditioning 
for minimizing total solution time.
For scaled problems, the relative advantage appears to increase 
as the number of processors is scaled upwards.
Euclid may also be used to good advantage as a smoother within 
multigrid methods.

%-----------------------------

\subsection{Overview}

Euclid is best thought of as an ``extensible ILU preconditioning
framework.''
{\em Extensible} means that Euclid can (and eventually will, time and
contributing agencies permitting) support many variants of ILU($k$)
and ILUT preconditioning.
(The current release includes Block Jacobi ILU($k$) and
Parallel ILU($k$) methods.)
Due to this extensibility, and also because Euclid was developed 
independently of the \hypre{} project, the methods by which one
passes runtime parameters to Euclid preconditioners
differ in some respects from the \hypre{} norm.
While users can directly set options within their code,
options can also be passed to Euclid preconditioners via
command line switches and/or small text-based configuration files.
The latter strategies have the advantage that users will not need to
alter their codes as Euclid's capabilities are extended.

%Euclid subscribes to the philosophy that
%the less coding required of the user (and the maintainer) the better.
%Hence, rather than coding interface function calls for every settable 
%parameter, 

The following fragment illustrates the minimum coding %that is
required to invoke Euclid preconditioning within \hypre{} application contexts.
The next subsection provides examples of the various ways in which
Euclid's options can be set. 
The final subsection lists the options,
and provides guidance as to the settings that (in our experience) 
will likely prove effective for minimizing execution time.

\begin{display}
\begin{verbatim}
#include "HYPRE_parcsr_ls.h"

HYPRE_Solver eu;
HYPRE_Solver pcg_solver;
HYPRE_ParVector b, x;
HYPRE_ParCSRMatrix A;

//Instantiate the preconditioner.
HYPRE_EuclidCreate(comm, &eu);

//Optionally use the following two calls to set runtime options.
// 1. pass options from command line or string array.
HYPRE_EuclidSetParams(eu, argc, argv);

// 2. pass options from a configuration file.
HYPRE_EuclidSetParamsFromFile(eu, "filename");

//Set Euclid as the preconditioning method for some
//other solver, using the function calls HYPRE_EuclidSetup
//and HYPRE_EuclidSolve.  We assume that the pcg_solver
//has been properly initialized.
HYPRE_PCGSetPrecond(pcg_solver,
                    (HYPRE_PtrToSolverFcn) HYPRE_EuclidSolve,
                    (HYPRE_PtrToSolverFcn) HYPRE_EuclidSetup,
                    eu);

//Solve the system by calling the Setup and Solve methods for, 
//in this case, the HYPRE_PCG solver.  We assume that A, b, and x
//have been properly initialized.
HYPRE_PCGSetup(pcg_solver, (HYPRE_Matrix)A, (HYPRE_Vector)b, (HYPRE_Vector)x);
HYPRE_PCGSolve(pcg_solver, (HYPRE_Matrix)parcsr_A, (HYPRE_Vector)b, (HYPRE_Vector)x);

//Destroy the Euclid preconditioning object.
HYPRE_EuclidDestroy(eu);

\end{verbatim}
\end{display}

%-----------------------------

\subsection{Setting options: examples}

For expositional purposes, assume you wish to set the ILU($k$)
factorization level to the value $k = 3$.
There are several methods of accomplishing this.
Internal to Euclid, options are stored in a simple database that
contains (name, value) pairs.
Various of Euclid's internal (private) functions query this
database to determine, at runtime, what action the user
has requested.
If you enter the option ``{\bf -eu\_stats 1''}, a report will
be printed when Euclid's destructor is called; this
report lists (among other statistics) the options that
were in effect during the factorization phase.

{\bf Method 1.}
By default, Euclid always looks for a file titled
``database'' in the working directory. 
If it finds such a file, it opens it and attempts to parse it as
a configuration file.
%In the directory in which you will execute the \hypre {}
%program, open a text file named ``database'' and insert the
Configuration files should be formatted as follows. 

\vspace{0.1in}
\indent {\tt >cat database} \\
\indent {\tt \#this is an optional comment} \\
\indent {\tt -level 3}
\vspace{0.1in}

Any line in a configuration file that contains a ``{\tt \#}''
character in the first column is ignored.
All other lines should begin with an option {\em name}, followed by
one or more blanks, followed by the option {\em value}.
Note that option names always begin with a ``-'' character.
If you include an option name that is not recognized by Euclid,
no harm should ensue.

{\bf Method 2.}
To pass options on the command line, call
\begin{display}
\begin{verbatim}
HYPRE_EuclidSetParams(HYPRE_Solver solver, int argc, char *argv[]);
\end{verbatim}
\end{display}
where {\tt argc} and {\tt argv} carry the usual connotation:
{\tt main(int argc, char *argv[])}.
If your \hypre{} application is called {\tt phoo}, you can
then pass options on the command line per the following example.

\begin{display}
\begin{verbatim}
mpirun -np 2 phoo -level 3
\end{verbatim}
\end{display}

Since Euclid looks for the ``database'' file when 
{\tt HYPRE\_EuclidCreate} is called, and parses the command line 
when {\tt HYPRE\_EuclidSetParams} is called,
option values passed on the command line will override 
any similar settings that may be contained in the ``database'' file.
Also, if same option name appears more than once on the command 
line, the final appearance determines the setting.

Some options, such as ``{\tt -bj}'' (see next subsection) are boolean.
Euclid always treats these options as the value ``1'' (true)
or ``0'' (false).  
When passing boolean options from the command line
the value may be committed, in which case it assumed to be ``1.''
Note, however, that when boolean options are contained in a
configuration file, either the ``1'' or ``0'' must
stated explicitly.

{\bf Method 3.}
There are two ways in which you can read in options from a file
whose name is other than ``database.''
First, you can call {\tt HYPRE\_EuclidSetParamsFromFile}
to specify a configuration filename.
Second, if you have passed the command line arguments as 
described above in Method 2, 
you can then specify the configuration filename on the command
line using the {\bf -db\_filename filename} option, e.g.,

\begin{display}
\begin{verbatim}
mpirun -np 2 phoo -db_filename ../myConfigFile
\end{verbatim}
\end{display}

%-----------------------------

\subsection{Options summary}

\begin{description}
\item[-level $\langle int \rangle$] Factorization level for ILU($k$).  
           Default: 1.
           Guidance: for 2D convection-diffusion and similar problems, 
           fastest solution time is typically obtained with levels 4 through
           8.  For 3D problems fastest solution time is typically 
           obtained with level 1.
                    
\item[-bj] Use Block Jacobi ILU preconditioning instead of PILU.
           Default: 0 (false). Guidance: if subdomains contain
           relatively few nodes (less than 1,000), or the problem is
           not well partitioned, Block Jacobi ILU 
           may give faster solution time than PILU.
\item[-eu\_stats] When Euclid's destructor is called a summary of
                 runtime settings and timing information is printed
                 to stdout.  Default: 0 (false).
                 The timing marks in the report are the maximum over 
                 all processors in the MPI communicator.
\item[-eu\_mem] When Euclid's destructor is called a summary of
               Euclid's memory usage is printed to stdout.
               Default: 0 (false).
               The statistics are for the processor whose rank 
               in MPI\_COMM\_WORLD is 0.
\item[-printTestData] This option is used in our autotest procedures,
                 and should not normally be invoked by users.
\end{description}

The following options are partially implemented, but not
yet fully functional (i.e, don't use them until further notice).

\begin{description}
\item[-sparseA $\langle float \rangle$] Drop-tolerance for ILU($k$) factorization.
                        Default: 0 (no dropping).
                        Entries are treated as zero if their absolute
                        value is less than \mbox{(sparseA * max)}, where ``max''
                        is the largest absolute value of any entry in the
                        row. Guidance: try this in conjunction with 
                        -rowScale.  CAUTION: If the coefficient matrix $A$ is 
                        symmetric, this
                        setting is likely to cause the filled matrix,
                        $F = L+U-I$, to be unsymmetric.
                        This setting has no effect when ILUT factorization
                        is selected.
\item[-rowScale] Scale values prior to factorization such that the
                 largest value in any row is +1 or -1.
                 Default: 0 (false).
                 CAUTION: If the coefficient matrix $A$ is symmetric, this 
                 setting is likely to cause the filled matrix,
                 $F = L+U-I$, to be unsymmetric.
                 Guidance: if the matrix is poorly scaled, turning on
                 row scaling may help convergence.
\item[-ilut $\langle float \rangle$] Use ILUT factorization instead
                 of the default, ILU($k$).  Here, $\langle float \rangle$
                 is the drop tolerance, which is relative to the largest 
                 absolute value of any entry in the row being factored.
                 CAUTION: If the coefficient matrix $A$ is symmetric, this 
                 setting is likely to cause the filled matrix,
                 $F = L+U-I$, to be unsymmetric.
\item[-maxNzPerRow  $\langle int \rangle$] This sets the maximum number
                 of nonzeros that is permitted in any row of $F$, in
                 addition to the number that would result from an ILU(0)
                 factorization.  A negative value %indicates that
                 indicates infinity (no limit).
                 %no limit should be imposed (equivalent to a setting
                 %if infinity).  
                 This setting is effective for both ILU($k$)
                 and ILUT factorization methods.
                 Default: infinity, for ILU($k$); 5, for ILUT.
\end{description}

%-----------------------------------------------------------------------------

\section{\pilut: Parallel Incomplete Factorization}
\label{PILUT}

Note that this code is no longer supported by the \hypre{} team. We recommend to 
use Euclid instead, which is more versatile and in general more efficient, especially
when used with many processors.

\pilut{} is a parallel preconditioner based on Saad's dual-threshold incomplete
factorization algorithm. The original version of \pilut{} was done by Karypis
and Kumar \cite{GKarypis_VKumar_1998} in terms of the Cray SHMEM library. The
code was subsequently modified by the \hypre{} team: SHMEM was replaced by MPI;
some algorithmic changes were made; and it was software engineered to be
interoperable with several matrix implementations, including \hypre{}'s ParCSR
format, PETSc's matrices, and ISIS++ RowMatrix. The algorithm produces an
approximate factorization $ L U$, with the preconditioner $M$ defined by $ M =
L U $.

{\bf Note:} \pilut{} produces a nonsymmetric preconditioner even when the
original matrix is symmetric. Thus, it is generally inappropriate for
preconditioning symmetric methods such as Conjugate Gradient.

%-----------------------------

\subsection*{Parameters:}

\begin{itemize}

\item
\code{SetMaxNonzerosPerRow( int LFIL ); (Default: 20)}
Set the maximum number of nonzeros to be retained in each row of $L$ and $U$.
This parameter can be used to control the amount of memory that $L$ and $U$
occupy. Generally, the larger the value of \code{LFIL}, the longer it takes to
calculate the preconditioner and to apply the preconditioner and the larger
the storage requirements, but this trades
off versus a higher quality preconditioner that reduces the number of
iterations.

\item
\code{SetDropTolerance( double tol ); (Default: 0.0001)}
Set the tolerance (relative to the 2-norm of the row) below which entries in L
and U are automatically dropped. \pilut{} first drops entries based on the drop
tolerance, and then retains the largest LFIL elements in each row that remain.
Smaller values of \code{tol} lead to more accurate preconditioners, but can
also lead to increases in the time to calculate the preconditioner.

\end{itemize}
